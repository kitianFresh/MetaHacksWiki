<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">
        <title>neural-networks-basics - MetaHacks Wiki</title>
        <meta name="keywords" content="wiki, simiki, computer, cognitive,"/>
        <meta name="description" content="my personal wiki"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ]
            }
        });
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
        <!--script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script!-->
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/">Home</a>&nbsp;»&nbsp;<a href="/#machinelearning">machinelearning</a>&nbsp;»&nbsp;<a href="/#machinelearning-deeplearning">deeplearning</a>&nbsp;»&nbsp;neural-networks-basics</div>
</div>
<div class="clearfix"></div>
<div id="title">neural-networks-basics</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#case-study">逻辑斯特回归 (Case Study)</a><ul>
<li><a href="#1-model">1 - 模型(Model)</a></li>
<li><a href="#2-cost">2 - 损失/策略(Cost)</a></li>
<li><a href="#3-algorithm">3 - 优化/学习 算法(Algorithm)</a><ul>
<li><a href="#31-forward-and-backward-propagation">3.1 - Forward and Backward propagation</a></li>
<li><a href="#32">3.2 优化算法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#shallow-neural-net">浅层神经网络(Shallow Neural Net)</a><ul>
<li><a href="#1-neural-network-model">1 - 模型 Neural Network model</a></li>
<li><a href="#2-">2 - 损失/策略</a></li>
<li><a href="#3-">3 - 学习/优化 算法</a><ul>
<li><a href="#31-forward-and-backward-propagation_1">3.1 Forward and Backward Propagation</a></li>
<li><a href="#32_1">3.2 参数更新公式</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#deep-neural-net">深度神经网络(Deep Neural Net)</a></li>
</ul>
</div>
<h1 id="case-study">逻辑斯特回归 (Case Study)</h1>
<p>task: 训练一个识别猫的LR</p>
<p><strong>输入数据的预处理:</strong><br />
 - <font color='blue'>Shape and Dimension 明确训练集/测试集 数据的形状和维度，比如数量和样本特征维度 (m_train, m_test, num_px, ...)</font><br />
 - <font color='blue'>Reshape 输入数据shape 使其成为一个合理输入 (num_px * num_px * 3, 1)</font><br />
 - <font color="blue">"Standardize" 数据归一化处理</font></p>
<h2 id="1-model">1 - 模型(Model)</h2>
<p>训练一个识别图片是否是猫的LR。一个简单的二分类器，直接使用 sigmoid 做输出层。<strong>Logistic Regression 就是一个简单的 Neural Network!</strong></p>
<p><img src="/static/images/ML/LR/LogReg_kiank.png" style="width:650px;height:400px;"></p>
<p><strong>模型的数学公式</strong>:</p>
<p>对某个输入数据 $x^{(i)}$, 下面是公式</p>
<p>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$<br />
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$ </p>
<h2 id="2-cost">2 - 损失/策略(Cost)</h2>
<p>二分类器可以直接使用 cross-entropy 交叉熵来定义损失函数:<br />
$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$<br />
$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$</p>
<h2 id="3-algorithm">3 - 优化/学习 算法(Algorithm)</h2>
<p>构建神经网络的基本步骤:<br />
1. 定义模型架构<br />
2. 初始化模型参数<br />
3. Loop:<br />
    - 前向传播计算损失 (forward propagation)<br />
    - 反向传播计算梯度 (backward propagation)<br />
    - 使用梯度更新参数 (gradient descent)基本上全部是梯度下降法，还有牛顿法</p>
<h3 id="31-forward-and-backward-propagation">3.1 - Forward and Backward propagation</h3>
<p>Forward Propagation:<br />
- 输入X<br />
- 计算激活 $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$<br />
- 计算损失: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</p>
<p>简单的求导公式即 Backward Propagation: </p>
<p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$<br />
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># FORWARD PROPAGATION (FROM X TO COST)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>              <span class="c"># compute activation</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">Y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>     <span class="c"># compute cost</span>

    <span class="c"># BACKWARD PROPAGATION (TO FIND GRAD)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,((</span><span class="n">A</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">dw</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">db</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">float</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">cost</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">())</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;dw&quot;</span><span class="p">:</span> <span class="n">dw</span><span class="p">,</span>
             <span class="s">&quot;db&quot;</span><span class="p">:</span> <span class="n">db</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">cost</span>
</pre></div>


<h3 id="32">3.2 优化算法</h3>
<p>优化过程就是通过最小化损失函数 $J$ 来学习参数 $w$ and $b$. 对参数 $\theta$, 优化规则是 $ \theta = \theta - \alpha \text{ } d\theta$, $\alpha$ 为学习速率 learning_rate.</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>

    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>


        <span class="c"># Cost and gradient calculation</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">propagate</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

        <span class="c"># Retrieve derivatives from grads</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;dw&quot;</span><span class="p">]</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;db&quot;</span><span class="p">]</span>

        <span class="c"># update rule</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">dw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span>  <span class="n">learning_rate</span><span class="o">*</span><span class="n">db</span>

        <span class="c"># Record the costs</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c"># Print the cost every 100 training examples</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="s">&quot;Cost after iteration </span><span class="si">%i</span><span class="s">: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;w&quot;</span><span class="p">:</span> <span class="n">w</span><span class="p">,</span>
              <span class="s">&quot;b&quot;</span><span class="p">:</span> <span class="n">b</span><span class="p">}</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;dw&quot;</span><span class="p">:</span> <span class="n">dw</span><span class="p">,</span>
             <span class="s">&quot;db&quot;</span><span class="p">:</span> <span class="n">db</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">costs</span>
</pre></div>


<h1 id="shallow-neural-net">浅层神经网络(Shallow Neural Net)</h1>
<h2 id="1-neural-network-model">1 - 模型 Neural Network model</h2>
<p>神经网络中的一个隐藏层节点其实是一个逻辑斯特回归分类器。</p>
<p><strong>model</strong>:</p>
<p><img src="/static/images/ML/DL/nn-basics/classification_kiank.png" style="width:600px;height:300px;"></p>
<p>单隐藏层的神经网络可以看成是以逻辑斯特回归为基础神经节点, 再多加一层再次向前传播而已. 隐藏层的每一个单元就可以看成是一个逻辑斯特回归,<br />
<img src="/static/images/ML/DL/nn-basics/nn1.png" style="width:600px;height:300px;"><br />
你可以把每个隐藏层节点都是一个逻辑斯特回归.<br />
<img src="/static/images/ML/DL/nn-basics/nn2.png" style="width:600px;height:300px;"><br />
下面用公式来表示一个前向传播的过程:<br />
<img src="/static/images/ML/DL/nn-basics/nn3.png" style="width:600px;height:300px;"><br />
<img src="/static/images/ML/DL/nn-basics/nn4.png" style="width:600px;height:300px;"><br />
整体架构：<br />
<img src="/static/images/ML/DL/nn-basics/nn-overview.png" style="width:600px;height:300px;"></p>
<p><strong>Mathematically</strong>:</p>
<p>$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}$$ <br />
$$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$<br />
$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}$$<br />
$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$</p>
<p>$$<br />
\begin{align}<br />
y^{(i)}_{prediction} = <br />
\begin{cases} <br />
1 &amp; \mbox{if } a^{[2] (i)} &gt; 0.5 \\<br />
0 &amp; \mbox{otherwise } <br />
\end{cases}<br />
\end{align}\tag{5}<br />
$$</p>
<h2 id="2-">2 - 损失/策略</h2>
<p>$$<br />
J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}<br />
$$</p>
<h2 id="3-">3 - 学习/优化 算法</h2>
<h3 id="31-forward-and-backward-propagation_1">3.1 Forward and Backward Propagation</h3>
<p>反向传播，使用链式法则和微分进行求导得到。这里关于向量求导和标量相比，最难的部分是 shape 和 导数形式问题。求导规则和标量一样，但是需要注意点乘和转置，以及求和形式。 这里有一个Trick是根据 dW 的shape 和 dimension 来推断出导数公式。</p>
<p><img src="/static/images/ML/DL/grad_summary.png" style="width:600px;height:300px;"></p>
<p>举个列子, 以下是 sigmoid 作为输出层构成的二分类器的交叉熵损失函数的向量形式, 改公式是 (6) 的向量形式：<br />
$$<br />
L = -\frac{1}{m}{Y\log(A)+ (1-Y)\log(1-A)} \qquad (7)<br />
$$</p>
<p>$$<br />
A<br />
=<br />
\begin{bmatrix}<br />
   a_1 \\<br />
   a_2 \\<br />
   \vdots \\<br />
   a_t \\<br />
   \vdots \<br />
   a_m<br />
\end{bmatrix} \quad<br />
Y<br />
=<br />
\begin{bmatrix}<br />
   y_1 &amp; y_2 &amp; \cdots &amp; y_t &amp; \cdots y_m<br />
\end{bmatrix}<br />
$$<br />
对 A 向量中的每个元素求导</p>
<p>$$<br />
d_a<br />
=<br />
\begin{bmatrix}<br />
   d_{a_1} \\<br />
   d_{a_2} \\<br />
   \vdots \\<br />
   d_{a_t} \\<br />
   \vdots \\<br />
   d_{a_m}<br />
\end{bmatrix}<br />
=<br />
\begin{bmatrix}<br />
   -\frac{1}{m}(\frac{y_1}{a_1}-\frac{1-y_1}{1-a_1}) \\<br />
   -\frac{1}{m}(\frac{y_2}{a_2}-\frac{1-y_2}{1-a_2}) \\<br />
   \vdots \\<br />
   -\frac{1}{m}(\frac{y_t}{a_t}-\frac{1-y_t}{1-a_t}) \\<br />
   \vdots \\<br />
   -\frac{1}{m}(\frac{y_m}{a_m}-\frac{1-y_m}{1-a_m})<br />
\end{bmatrix}<br />
$$</p>
<p>以上的标量形式累积成一个向量写法之后，就是以下公式：</p>
<p>$$<br />
\begin{align}<br />
d_{a^{L}} &amp;= \frac{\partial L}{\partial A} &amp; \\<br />
&amp;= -\frac{1}{m}(\frac{Y^T}{A}-\frac{1-Y^T}{1-A}) &amp; \\<br />
&amp;= \frac{1}{m}{\frac{A-Y^T}{A*(1-A)}} \qquad (3)<br />
\end{align}<br />
$$</p>
<p>其中的 $A<em>(1-A)$ 以及 $(A-Y^T)/A</em>(1-A)$ 中的 $*/$都是element-wise 的矩阵运算;</p>
<p>$$<br />
\begin{align}<br />
d_{z^{L}} &amp;= \frac{\partial L}{\partial A}\frac{\partial A}{\partial Z} &amp; \\<br />
&amp;= d_{a}A*(1-A) &amp; \\<br />
&amp;= \frac{1}{m}(A-Y^T) \qquad (4)<br />
\end{align}<br />
$$</p>
<p>消除后可以化简, 这里的 A 是最后一层的激活输出;</p>
<p>$$<br />
\begin{align}<br />
d_{w^{L}} &amp;= \frac{\partial L}{\partial Z}\frac{\partial Z}{\partial W} &amp; \\<br />
&amp;= d_{z^{L}}(A^{L-1})^T \qquad (5)<br />
\end{align}<br />
$$</p>
<p>为了保持得到的导数矩阵的维度和$W$ 保持一致,我们必须对链式法则求导的结果运算进行转置和交换,来保证维度的一致性; 上式中 $\frac{\partial Z}{\partial W}$ 得到 $A^{L-1}$, $\frac{\partial L}{\partial Z}$ 得到 $d_z$, 由于 $d_w$ 的维度应该是 $(n_{L}, n_{L-1})$, 而 $d_z$ 的维度是 $(n_{L}, m)$, $A^{L-1}$ 的维度是 $(n_{L-1}, m)$, 因此我们推断出 $d_w = d_{z^{L}}(A^{L-1})^T$ 才合理。</p>
<p>最后，偏移量的导数形式很奇怪，会多出一个求和。原因是 b（如果是scalar） 的系数可以看成是全一的矩阵[[1,1,1,...1]],其系数在累计 m 个例子的误差的时候，被求和了，因此会出现 求和公式。比如 $b^{L}$ 的系数shape 是 $(n_{L}, 1)$, 而在求Z的时候，b会被广播成 $(n_L, m)$, 然后计算损失的时候会在m维度上累加m次，因此出现了 累加 m次。如果无法理解，可以直接记住这个公式。反向传播都一样。</p>
<p>$$<br />
\begin{align}<br />
d_{b^{L}} &amp;= \frac{\partial L}{\partial Z}\frac{\partial Z}{\partial b} &amp; \\<br />
&amp;= \sum\limits_{i = 0}^{m}{d_{z^{L}}} \qquad (5)<br />
\end{align}<br />
$$</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;W2&quot;</span><span class="p">]</span>

    <span class="c"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">&quot;A1&quot;</span><span class="p">]</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">&quot;A2&quot;</span><span class="p">]</span>

    <span class="c"># Backward propagation: calculate dW1, db1, dW2, db2. </span>
    <span class="n">dZ2</span><span class="o">=</span> <span class="n">A2</span> <span class="o">-</span> <span class="n">Y</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span>
             <span class="s">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span><span class="p">,</span>
             <span class="s">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span>
             <span class="s">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">grads</span>
</pre></div>


<h3 id="32_1">3.2 参数更新公式</h3>
<p>$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m}(a^{[2] (i)} - y^{(i)}) $</p>
<p>$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $</p>
<p>$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}} $</p>
<p>$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2})$</p>
<p>$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $</p>
<p>$\frac{\partial \mathcal{J} }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}} $</p>
<ul>
<li>符号 $*$ 表示 elementwise 乘积.</li>
<li>偏导和微分等价:<ul>
<li>$dW1 = \frac{\partial \mathcal{J} }{ \partial W_1 }$</li>
<li>$db1 = \frac{\partial \mathcal{J} }{ \partial b_1 }$</li>
<li>$dW2 = \frac{\partial \mathcal{J} }{ \partial W_2 }$</li>
<li>$db2 = \frac{\partial \mathcal{J} }{ \partial b_2 }$</li>
</ul>
</li>
</ul>
<p>参数更新，mini-BGD<br />
  - $W2 = W2 - \lambda * dW2$<br />
  - $b2 = b2 - \lambda * db2$<br />
  - $W1 = W1 - \lambda * dW1$<br />
  - $b1 = b1 - \lambda * db1$</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">):</span>
    <span class="c"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">&quot;b2&quot;</span><span class="p">]</span>

    <span class="c"># Retrieve each gradient from the dictionary &quot;grads&quot;</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;dW1&quot;</span><span class="p">]</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;db1&quot;</span><span class="p">]</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;dW2&quot;</span><span class="p">]</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">&quot;db2&quot;</span><span class="p">]</span>

    <span class="c"># Update rule for each parameter</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW1</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db1</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW2</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db2</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>


<h1 id="deep-neural-net">深度神经网络(Deep Neural Net)</h1>
<p>$$<br />
\begin{align}<br />
f(ab)&amp;=(ab)^2 &amp;&amp; (\text{by definition of $f$})\\<br />
&amp;=(ab)(ab)\\<br />
&amp;=a^2 b^2 &amp;&amp; (\text{since $G$ is abelian})\\<br />
&amp;=f(a)f(b) &amp;&amp; (\text{by definition of $f$}).<br />
\end{align}<br />
$$</p>
<p>$$<br />
A=<br />
\begin{bmatrix}<br />
  \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} \\[6pt]<br />
   \frac{2}{3} &amp;\frac{-1}{3} &amp;\frac{-1}{3} \\[6pt]<br />
   \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{-2}{3}<br />
\end{bmatrix}<br />
$$</p>
<p>$$<br />
    \begin{matrix}<br />
    1 &amp; x &amp; x^2 \\<br />
    1 &amp; y &amp; y^2 \\<br />
    1 &amp; z &amp; z^2 \\<br />
    \end{matrix}<br />
$$</p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2018-02-07 21:59 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'neural-networks-basics',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 田奇.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/kitianFresh/MetaHacksWiki/tree/master" target="_blank"> github </a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/tipuesearch_content.js"></script>
        <script src="/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>