<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">
        <title>word-embeddings - MetaHacks Wiki</title>
        <meta name="keywords" content="wiki, simiki, computer, cognitive,"/>
        <meta name="description" content="my personal wiki"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ]
            }
        });
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
        <!--script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script!-->
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-114706319-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-114706319-1');
        </script>
        
        <!-- Baidu Analytics -->
        <script>
            var _hmt = _hmt || [];
            (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?6e445c332d0cb95f356894a8d3b9f545";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
            })();
        </script>


    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/">Home</a>&nbsp;»&nbsp;<a href="/#machinelearning">machinelearning</a>&nbsp;»&nbsp;<a href="/#machinelearning-deeplearning">deeplearning</a>&nbsp;»&nbsp;word-embeddings</div>
</div>
<div class="clearfix"></div>
<div id="title">word-embeddings</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#word-embeddings">Word Embeddings</a><ul>
<li><a href="#intuition">Intuition</a></li>
<li><a href="#word-represention">word represention</a></li>
<li><a href="#embedding-matrix">embedding matrix</a><ul>
<li><a href="#word2vec">为什么Word2Vec 不使用正则项？</a></li>
<li><a href="#word2vec_1">为什么Word2Vec 隐藏层没有使用激活函数？</a></li>
<li><a href="#word2vec-wt-w">Word2Vec 两个矩阵能够使用同一个吗？也就是说 W.T = W'</a></li>
<li><a href="#_1">为什么取第一个矩阵做词向量而不是第二个矩阵？</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#learning-word-embeddings">Learning Word Embeddings</a><ul>
<li><a href="#word2vec_2">word2vec</a></li>
<li><a href="#hierarchical-softmax-negative-sampling-for-speeding-training">hierarchical softmax &amp; negative sampling for speeding training</a></li>
<li><a href="#glove-word-vectors">GloVe word vectors</a></li>
<li><a href="#_2">参考资料</a></li>
</ul>
</li>
<li><a href="#application">Application</a><ul>
<li><a href="#sentiment-classification">sentiment classification</a><ul>
<li><a href="#1-">1 - 简单模型</a></li>
<li><a href="#2-rnn">2 - 基于RNN的序列模型</a></li>
</ul>
</li>
<li><a href="#debiasing-word-embeddings">debiasing word embeddings</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="word-embeddings">Word Embeddings</h1>
<h2 id="intuition">Intuition</h2>
<p>在介绍词向量之前，我们先说如何在计算机中表示一个词。</p>
<p>不论是哪一国语言，如果我们考虑请语言学家，根据语法和规则来学习某门语言，由于每种语言有他的语法，耗费的人力成本和时间成本非常高，并且基于语法和规则的翻译，系统也是很难编写的。Google 就是一个例子，他们每开除一名语言学家，机器翻译准确率就会提升。尽管不同语言有不同的语法，但是他们都有更抽象abstract和更加通用general的结构能够被学习到，那是什么呢，你会发现，大多数语言都是由字符构成单词（词素，词素再构成单词un+fortunate+ly）（汉语，日语，韩语可能是基本笔画构成字），词构成句子，句子构成段落，段落构成文章。然后从语义上来讲，不同语言相同语义的某些词，他必定出现在不同语言相同语义的上下文，什么意思呢？比如，"机器学习是实现人工智能的最好的方法之一"，"machine learning is one of best ways of achieving artificial intelligience"。不管是哪一种语言，机器学习(machine learning)可能往往出现在人工智能(artificial intelligience)的上下文。这些都是可以学习的隐含pattern。你会发现，一旦你找到了一种可以学习这些pattern的模型，那么你就找到了更加通用的解决方案，不同的语言，使用语料库进行训练即可得到模型，不需要和传统的基于规则和语法的编程一样，每次换一门语言，需要重新学习规则，重新编写系统。</p>
<p>那什么是词向量呢？我们知道单词是所有语言的基础，所以表征出单词的含义，我们就成功了一半。我们需要判断两个词语是否同义或者同样的词性（语义和语法），有什么好的方法呢？暂停30s想一下，我们如何学习英语的同义词呢？我记得上高中的时候，我经常通过把一组同义词放在一起学习，背诵和比较，机器能这样做么？可以，但是得人工收集和分类很多不同的词语，然后对这些词语做标记，同一个意思的词语都是同样的标记，比如他们都是动词，都是喜欢的意思，让后存储在计算机中，直接查询？这太复杂了。。。。所以有大神尽然神奇的想到把一个词语使用向量来表示，what，fuck，这可以吗？学习计算机的同学可能知道，这样存储对计算机来讲太爽了，很容易设计数据结构和运算。但是，这是什么意义？能够有效果么？如何表示成向量？接下来我们就来看看到底是怎么做到的，为啥有效果，而且还秒杀传统的方法。</p>
<p>再次回到我开头说的，从语义上来讲，每一种语言的相同语义的某些词，他必定出现在不同语言相同语义的上下文，这些都是可以学习的隐含pattern。如果两个词是同义词，那是不是这两个词会很大的可能性出现在相同的上下文。然后我突然顿悟到，这不就是我考研时候使用的方法么，单词不认识，通过上下文推出他可能是某个词的同义词，只是没学过而已。我们会通过脑海里原始的一些pattern（我们见过的相似的句子以及上下文）推理出来，这个词应该就是那个词的意思！顿时感觉，大神的思路也是非常平凡的，难就难在，人家就给你做出来了。</p>
<p>然后，为什么要用向量来表示一个词语呢？有意义么？答案当然是有的！我们知道，向量是一组不同的元素构成的，我们最开始学习的向量，其实是物理中的矢量，就是带方向的，通过(x,y,z) 来分别指明三个方向，向量就是更高维度的矢量了，他的每一个维度都是有具体含义的。一个词是不是有不同的属性和特点，比如他是消极词的概率，他使用的频率，他是副词的概率，他是动词的概率等等。因此，向量可以比较完善的表示出一个词的意义和结构。</p>
<p>接下来，就是通过神经网络玄学来试试，能不能自动学习出一个向量，虽然我不知道这个向量每个维度到底是啥意思。</p>
<p>词嵌入向量具有很强的泛化和迁移学习能力，即使使用很小的训练集学习出来的词向量，也很容易应用与其他词。</p>
<h2 id="word-represention">word represention</h2>
<h2 id="embedding-matrix">embedding matrix</h2>
<h3 id="word2vec">为什么Word2Vec 不使用正则项？</h3>
<p>因为Word2Vec的目的并不是为了让这个模型去适应语料库意外的语料，也不是用来再去预测其他的未见过测用例，他只是为了训练出当前语料库中所有单词的词向量。只需要拟合当前数据即可，不需要泛化。</p>
<h3 id="word2vec_1">为什么Word2Vec 隐藏层没有使用激活函数？</h3>
<p>目的不是为了泛化，也不需要激活，激活是为了</p>
<h3 id="word2vec-wt-w">Word2Vec 两个矩阵能够使用同一个吗？也就是说 W.T = W'</h3>
<p>目前网上没人思考过这个问题，如果我们得到了第一个矩阵是词嵌入矩阵，他的每一行代表的是对应单词的词向量，然后采用计算隐层向量（他其实就是某个词向量）和其他所有词向量的距离，用这个距离来度量输入词和其他词之间的关系，以此再进行softmax概率分布来评估误差，但是你不能还是用第一个矩阵来学习，因为第一个矩阵是用来学习词嵌入的，第二个矩阵是用来学习词嵌入和他的上下文单词关系的，这是两个不同的学习对象，因此我们需要再引入一个新矩阵来学习。</p>
<p>目前我认为如果不能的话，应该是这个原因。但是如果可以的话，我觉得也说得通，这得做实验了。</p>
<h3 id="_1">为什么取第一个矩阵做词向量而不是第二个矩阵？</h3>
<p>目前网上没人思考过这个问题，目前的解释认为从one-hot 输入到隐层其实是编码进入词向量空间，第一层映射是真实的拿到词向量，也就是说第一个矩阵才是学习我们需要的词向量的矩阵，从隐层到输出是解码到one-hot，但是第二层映射解码出来的目的不是变会原来输入单词的one-hot,而是尽可能匹配出上下文单词的one-hot。也即是说第二层映射过程中发生的是距离计算和度量，第二个矩阵学习到的是某个词向量和他上下文距离远近的关系。</p>
<h1 id="learning-word-embeddings">Learning Word Embeddings</h1>
<h2 id="word2vec_2">word2vec</h2>
<h2 id="hierarchical-softmax-negative-sampling-for-speeding-training">hierarchical softmax &amp; negative sampling for speeding training</h2>
<h2 id="glove-word-vectors">GloVe word vectors</h2>
<h2 id="_2">参考资料</h2>
<ul>
<li>论文</li>
<li><a href="http://wiki.hacksmeta.com/static/pdf/A-Neural-Probabilistic-Language-Model-1.pdf">A-Neural-Probabilistic-Language-Model-1</a></li>
<li><a href="http://wiki.hacksmeta.com/static/pdf/Efficient-Estimation-of-Word-Representations-2.pdf">Efficient-Estimation-of-Word-Representations</a></li>
<li><a href="http://wiki.hacksmeta.com/static/pdf/5021-distributed-representations-of-words-and-phrases-and-their-compositionality-3.pdf">5021-distributed-representations-of-words-and-phrases-and-their-compositionality</a></li>
<li><a href="http://wiki.hacksmeta.com/static/pdf/GloVe-Global-Vectors-for-Word-Representation-4.pdf">GloVe-Global-Vectors-for-Word-Representation</a></li>
<li><a href="http://wiki.hacksmeta.com/static/pdf/word2vec-Parameter-Learning-Explained-5.pdf">word2vec-Parameter-Learning-Explained</a></li>
<li>博客</li>
<li><a href="https://mp.weixin.qq.com/s/aeoFx6sIX6WNch51XRF5sg">秒懂词向量Word2vec的本质</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型</a></li>
<li><a href="http://blog.csdn.net/han____shuai/article/details/50882135">深度学习word2vec笔记之基础篇算法篇应用篇--写的非常到位</a></li>
<li><a href="http://blog.idejie.com/2017/08/13/word-embeding/">词嵌入的直觉理解：从计数向量到 Word2Vec</a></li>
<li>实现</li>
<li><a href="https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac">Learn Word2Vec by implementing it in tensorflow</a></li>
<li><a href="http://blog.csdn.net/u014595019/article/details/51884529">自己动手写word2vec</a></li>
<li><a href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/">Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)</a></li>
<li><a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/">Word2Vec word embedding tutorial in Python and TensorFlow</a></li>
<li>Word2Vec Resources</li>
<li><a href="http://mccormickml.com/2016/04/27/word2vec-resources/">Word2Vec Resources</a></li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型</a></p>
</li>
<li>
<p>Projects</p>
</li>
<li><a href="http://zake7749.github.io/2016/12/17/how-to-develop-chatbot/">聊天机器人的开发思路</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26729228">新闻上的文本分类：机器学习大乱斗</a></li>
<li><a href="https://jizhi.im/index">jizhi</a></li>
<li><a href="https://biendata.com/">biendata</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27424282">分分钟带你杀入Kaggle Top 1%</a></li>
<li>文本分类和标签</li>
<li><a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">Hierarchical Attention Networks for Document Classification</a></li>
<li><a href="https://cs224d.stanford.edu/reports/BergerMark.pdf">Large Scale Multi-label Text Classification with Semantic Word Vectors</a></li>
<li><a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">Recurrent Neural Network for Text Classification with Multi-Task Learning</a></li>
<li><a href="https://arxiv.org/pdf/1510.03820.pdf">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a></li>
<li>
<p><a href="https://arxiv.org/abs/1509.01626">Character-level Convolutional Networks for Text Classification</a></p>
</li>
<li>
<p><a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">Implementing a CNN for Text Classification in TensorFlow</a></p>
</li>
</ul>
<p>https://github.com/Qinbf/Tensorflow/blob/master/Tensorflow%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E4%B8%8E%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%BA%94%E7%94%A8/%E7%A8%8B%E5%BA%8F/data_handle.ipynb</p>
<p>就是如何判断一篇文章是否抄袭了另一篇文章，哪里抄袭了。如果是人工去判断的话，非常简单。我们一般会先扫描一遍，发现有很多重复的段落和句子，基本就可以判定他们很相似了。但是如果这两篇文章表达的是同样的内容，但是很多词被替换了，语句顺序也被打乱了，这个时候，光靠扫描句子词已经无法判断了，我们得看语义。这是我们人工的做法，具体到计算机，它要如何识别这种相似呢？我们也从人工上最简单的方法开始，文档基本上就是词构成的序列，如果我们比较这两个序列的差异（计算机里面有个算法叫最小编辑距离），就可以大致得到他们是否在字面上相似。但是当面临第二个问题的时候，这种做法就失效了。</p>
<p>第二个问题，最简单的办法是TF-IDF，把每一个文档看成是一个TF-IDF构成的向量，计算向量之间的距离来判断。TF-IDF靠的是词频统计，就是我们做一个词汇表，然后每一篇文章都是一个这种词汇表构成的向量，向量的每一个值是这个词在该文章中出现的统计词频。但是这种方法其实只是大致估算了一个文章中词的含量信息，没有给出更加具体的语义和语序比较，因此往往也不能有很高的识别率。</p>
<p>第二种方法就是分类器，如果两篇文章的作者都是丰富的写手，他们以前写过很多文章的话，我们可以做分类器，就是作者1的文章是一类，作者2的文章是一类，如果作者2的新文章被分到作者1的类，那这篇文章可能就是抄袭的。</p>
<h1 id="application">Application</h1>
<h2 id="sentiment-classification">sentiment classification</h2>
<p>输入一个句子，输出该句子的情感分类。</p>
<h3 id="1-">1 - 简单模型</h3>
<p><center><br />
<img src="/static/images/ML/DL/rnn/Emojifier-V1.png" style="width:900px;height:300px;"><br />
<caption><center> <strong>Figure</strong>: Baseline model (Emojifier-V1).</center></caption><br />
</center><br />
不考虑单词顺序，将词向量简单平均后扔进普通神经网络做多分类。该方法简单粗暴，无法考虑句子中词语的反义即顺序关系。比如，they are lacking good taste, good service and good postion. 这句话很容易在这种网络中被认为是好的类。因为出现了很多 good. 由于未考虑时序关系，lacking good 导致。<br />
$$ z^{(i)} = W . avg^{(i)} + b$$<br />
$$ a^{(i)} = softmax(z^{(i)})$$<br />
$$ \mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$</p>
<h3 id="2-rnn">2 - 基于RNN的序列模型</h3>
<p>考虑单词顺序对句子的影响。many-to-one 的 rnn 模型。rnn 会考虑前后单词之间的关系，因此 rnn 是更好的选择。<br />
由于不同的输入sentence长度不一样，如果不对输入句子做特殊处理，就无法并行计算所有句子。这里采用的方法就是 padding.<br />
<center><br />
<img src="/static/images/ML/DL/rnn/emojifier-v2.png" style="width:900px;height:300px;"><br />
<caption><center> <strong>Figure</strong>: Emojifier-V2. A 2-layer LSTM sequence classifier.</center></caption><br />
</center></p>
<div class="hlcode"><pre><span class="c"># GRADED FUNCTION: sentences_to_indices</span>

<span class="k">def</span> <span class="nf">sentences_to_indices</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">max_len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span>
<span class="sd">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span>

<span class="sd">    Arguments:</span>
<span class="sd">    X -- array of sentences (strings), of shape (m, 1)</span>
<span class="sd">    word_to_index -- a dictionary containing the each word mapped to its index</span>
<span class="sd">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span>

<span class="sd">    Returns:</span>
<span class="sd">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                                   <span class="c"># number of training examples</span>

    <span class="c">### START CODE HERE ###</span>
    <span class="c"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span>
    <span class="n">X_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">max_len</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>                               <span class="c"># loop over training examples</span>

        <span class="c"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span>
        <span class="n">sentence_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">)]</span>

        <span class="c"># Initialize j to 0</span>
        <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c"># Loop over the words of sentence_words</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence_words</span><span class="p">:</span>
            <span class="c"># Set the (i,j)th entry of X_indices to the index of the correct word.</span>
            <span class="n">X_indices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="s">&#39;&#39;</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="c"># Increment j to j + 1</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">X_indices</span>
</pre></div>


<p>使用keras 实现 word embedding layer<br />
<center><br />
<img src="/static/images/ML/DL/rnn/embedding1.png" style="width:900px;height:300px;"><br />
<caption><center> <strong>Figure</strong>: embedding.</center></caption><br />
</center></p>
<div class="hlcode"><pre><span class="c"># GRADED FUNCTION: pretrained_embedding_layer</span>

<span class="k">def</span> <span class="nf">pretrained_embedding_layer</span><span class="p">(</span><span class="n">word_to_vec_map</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span>
<span class="sd">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span>

<span class="sd">    Returns:</span>
<span class="sd">    embedding_layer -- pretrained layer Keras instance</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vocab_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>                  <span class="c"># adding 1 to fit Keras embedding (requirement)</span>
    <span class="n">emb_dim</span> <span class="o">=</span> <span class="n">word_to_vec_map</span><span class="p">[</span><span class="s">&quot;cucumber&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>      <span class="c"># define dimensionality of your GloVe word vectors (= 50)</span>

    <span class="c">### START CODE HERE ###</span>
    <span class="c"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span>
    <span class="n">emb_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_len</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">))</span>

    <span class="c"># Set each row &quot;index&quot; of the embedding matrix to be the word vector representation of the &quot;index&quot;th word of the vocabulary</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">emb_matrix</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">word_to_vec_map</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>

    <span class="c"># Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. </span>
    <span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_len</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="c">### END CODE HERE ###</span>

    <span class="c"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the &quot;None&quot;.</span>
    <span class="n">embedding_layer</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="bp">None</span><span class="p">,))</span>

    <span class="c"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span>
    <span class="n">embedding_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">emb_matrix</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">embedding_layer</span>


<span class="c"># GRADED FUNCTION: Emojify_V2</span>

<span class="k">def</span> <span class="nf">Emojify_V2</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">word_to_vec_map</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function creating the Emojify-v2 model&#39;s graph.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    input_shape -- shape of the input, usually (max_len,)</span>
<span class="sd">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span>
<span class="sd">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span>

<span class="sd">    Returns:</span>
<span class="sd">    model -- a model instance in Keras</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c">### START CODE HERE ###</span>
    <span class="c"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype &#39;int32&#39; (as it contains indices).</span>
    <span class="n">sentence_indices</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">&#39;int32&#39;</span><span class="p">)</span>

    <span class="c"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span>
    <span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">pretrained_embedding_layer</span><span class="p">(</span><span class="n">word_to_vec_map</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>

    <span class="c"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sentence_indices</span><span class="p">)</span> 

    <span class="c"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span>
    <span class="c"># Be careful, the returned output should be a batch of sequences.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">return_sequences</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="c"># Add dropout with a probability of 0.5</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="c"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span>
    <span class="c"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="c"># Add dropout with a probability of 0.5</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="c"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="c"># Add a softmax activation</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>

    <span class="c"># Create Model instance which converts sentence_indices into X.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sentence_indices</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>

    <span class="c">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">maxLen = 20</span>
<span class="sd">model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)</span>
<span class="sd">model.summary()</span>
<span class="sd">model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])</span>
<span class="sd">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span>
<span class="sd">Y_train_oh = convert_to_one_hot(Y_train, C = 5)</span>
<span class="sd">model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)</span>
<span class="sd">X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)</span>
<span class="sd">Y_test_oh = convert_to_one_hot(Y_test, C = 5)</span>
<span class="sd">loss, acc = model.evaluate(X_test_indices, Y_test_oh)</span>
<span class="sd">print()</span>
<span class="sd">print(&quot;Test accuracy = &quot;, acc)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>


<div class="hlcode"><pre><span class="err">&#39;\</span><span class="n">nmaxLen</span> <span class="o">=</span> <span class="mi">20</span><span class="err">\</span><span class="n">nmodel</span> <span class="o">=</span> <span class="n">Emojify_V2</span><span class="p">((</span><span class="n">maxLen</span><span class="p">,),</span> <span class="n">word_to_vec_map</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span><span class="err">\</span><span class="n">nmodel</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span><span class="err">\</span><span class="n">nmodel</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="err">\&#39;</span><span class="n">categorical_crossentropy</span><span class="err">\&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="err">\&#39;</span><span class="n">adam</span><span class="err">\&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="err">\&#39;</span><span class="n">accuracy</span><span class="err">\&#39;</span><span class="p">])</span><span class="err">\</span><span class="n">nX_train_indices</span> <span class="o">=</span> <span class="n">sentences_to_indices</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">maxLen</span><span class="p">)</span><span class="err">\</span><span class="n">nY_train_oh</span> <span class="o">=</span> <span class="n">convert_to_one_hot</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span><span class="err">\</span><span class="n">nmodel</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_indices</span><span class="p">,</span> <span class="n">Y_train_oh</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="err">\</span><span class="n">nX_test_indices</span> <span class="o">=</span> <span class="n">sentences_to_indices</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="n">maxLen</span><span class="p">)</span><span class="err">\</span><span class="n">nY_test_oh</span> <span class="o">=</span> <span class="n">convert_to_one_hot</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span><span class="err">\</span><span class="n">nloss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test_indices</span><span class="p">,</span> <span class="n">Y_test_oh</span><span class="p">)</span><span class="err">\</span><span class="n">nprint</span><span class="p">()</span><span class="err">\</span><span class="n">nprint</span><span class="p">(</span><span class="s">&quot;Test accuracy = &quot;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span><span class="err">\</span><span class="n">n</span><span class="err">&#39;</span>
</pre></div>


<h2 id="debiasing-word-embeddings">debiasing word embeddings</h2>
</div>
<div id="content-footer">
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多</p>
<table>
  <tr><td><img src="/static/images/My/WeChatPay.jpeg" style="width:200px;height:200px;"></td>
  <td><img src="/static/images/My/AliPay.jpeg" style="width:200px;height:200px;"></td></tr>
</table>created in <span class="create-date date"> 2018-02-23 13:01 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: 'word-embeddings',
  title: 'word-embeddings',
  owner: 'kitianFresh',
  repo: 'MetaHacksWiki',
  oauth: {
    client_id: '759b6fcf793dbef4e7a0',
    client_secret: '3c8fcf8b0a76c4acfc07b01a97e4f55f4c6ecbbd',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 田奇.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/kitianFresh/MetaHacksWiki/tree/master" target="_blank"> github </a>.
            </span>
        </div>
        

        <script src="/tipuesearch_content.js"></script>
        <script src="/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>